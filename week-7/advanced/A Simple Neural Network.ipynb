{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(from http://iamtrask.github.io/2015/07/12/basic-python-network/)\n",
    "\n",
    "A neural network trained with backpropagation is attempting to use input to predict output.\n",
    "\n",
    "\n",
    "|Input 1| Input 2 | Input 3| Output |\n",
    "|---|---|---|---|\n",
    "| 0 | 0 | 1 | 0 |\n",
    "| 1 | 1 | 1 | 1 |\n",
    "| 1 | 0 | 1 | 1 |\n",
    "| 0 | 1 | 1 | 0 |\n",
    "\n",
    "Consider trying to predict the output column given the three input columns. We could solve this problem by simply measuring statistics between the input values and the output values. If we did so, we would see that the leftmost input column is perfectly correlated with the output. Backpropagation, in its simplest form, measures statistics like this to make a model. Let's jump right in and use it to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def nonlin(x, deriv = False):\n",
    "    if(deriv == True):\n",
    "        return x * (1-x)\n",
    "    return 1/(1+np.exp(-1*x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our \"nonlinearity\". While it can be several kinds of functions, this nonlinearity maps a function called a \"sigmoid\". A sigmoid function maps any value to a value between 0 and 1. We use it to convert numbers to probabilities. It also has several other desirable properties for training neural networks.\n",
    "\n",
    "Notice that this function can also generate the derivative of a sigmoid (when deriv=True). One of the desirable properties of a sigmoid function is that its output can be used to create its derivative. If the sigmoid's output is a variable \"out\", then the derivative is simply out * (1-out). This is very efficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in\n",
    "X = np.array([[0,0,1],\n",
    "              [0,1,1],\n",
    "              [1,0,1],\n",
    "              [1,1,1]])\n",
    "\n",
    "# out\n",
    "y = np.array([[0,0,1,1]]).T\n",
    "\n",
    "# seed random numbers to make calculation deterministic\n",
    "np.random.seed(1)\n",
    "\n",
    "# initialize weights randomly with mean 0\n",
    "syn0 = 2*np.random.random((3,1)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This initializes our input dataset as a numpy matrix. Each row is a single \"training example\". Each column corresponds to one of our input nodes. Thus, we have 3 input nodes to the network and 4 training examples.\n",
    "\n",
    "The next lime initializes our output dataset. In this case, I generated the dataset horizontally (with a single row and 4 columns) for space. \".T\" is the transpose function. After the transpose, this y matrix has 4 rows with one column. Just like our input, each row is a training example, and each column (only one) is an output node. So, our network has 3 inputs and 1 output.\n",
    "\n",
    "It's good practice to seed your random numbers. Your numbers will still be randomly distributed, but they'll be randomly distributed in exactly the same way each time you train. This makes it easier to see how your changes affect the network.\n",
    "\n",
    "syn0 is our weight matrix for this neural network. It's called \"syn0\" to imply \"synapse zero\". Since we only have 2 layers (input and output), we only need one matrix of weights to connect them. Its dimension is (3,1) because we have 3 inputs and 1 output. Another way of looking at it is that l0 is of size 3 and l1 is of size 1. Thus, we want to connect every node in l0 to every node in l1, which requires a matrix of dimensionality (3,1). :) \n",
    "\n",
    "Also notice that it is initialized randomly with a mean of zero. There is quite a bit of theory that goes into weight initialization. For now, just take it as a best practice that it's a good idea to have a mean of zero in weight initialization. \n",
    "\n",
    "Another note is that the \"neural network\" is really just this matrix. We have \"layers\" l0 and l1 but they are transient values based on the dataset. We don't save them. All of the learning is stored in the syn0 matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for iter in xrange(10000):\n",
    "    # forward propogation\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0, syn0))\n",
    "    \n",
    "    # what's our error - how far off are we?\n",
    "    l1_error = y - l1\n",
    "    \n",
    "    # multiply how much we missed by the\n",
    "    # slope of the sigmoid at the values in l1\n",
    "    l1_delta = l1_error * nonlin(l1, True)\n",
    "    \n",
    "    # update weights\n",
    "    syn0 += np.dot(l0.T, l1_delta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
